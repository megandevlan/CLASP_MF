{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca5fbcb8-a2c2-41b1-a977-b989a91ece13",
   "metadata": {},
   "source": [
    "## STATUS: \n",
    "- Couldn't get this to work :( "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adfb7e85-fc59-43fa-be34-a9e32a223675",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692da106-9d48-4f5b-8f88-5345ed9fcf5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32ae0eb6-263e-4be9-8376-3849962eb531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "\n",
    "# # Plotting utils \n",
    "import datetime\n",
    "import glob\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import warnings\n",
    "from datetime import date, timedelta\n",
    "import Ngl\n",
    "import cartopy\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "import cartopy.util\n",
    "import matplotlib as matplotlib\n",
    "import matplotlib.animation as animation\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.patches as patches\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "import metpy.calc as mpc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "import xarray as xr\n",
    "from matplotlib.dates import DateFormatter\n",
    "from metpy.units import units\n",
    "from metpy import interpolate\n",
    "from metpy.calc import vertical_velocity\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b31f40f-d72f-4d8a-a4b9-f87b90776540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grabbed from Brian M. to use time midpoints, not end periods\n",
    "def cesm_correct_time(ds):\n",
    "    \"\"\"Given a Dataset, check for time_bnds,\n",
    "       and use avg(time_bnds) to replace the time coordinate.\n",
    "       Purpose is to center the timestamp on the averaging inverval.   \n",
    "       NOTE: ds should have been loaded using `decode_times=False`\n",
    "    \"\"\"\n",
    "    assert 'time_bnds' in ds\n",
    "    assert 'time' in ds\n",
    "    correct_time_values = ds['time_bnds'].mean(dim='nbnd')\n",
    "    # copy any metadata:\n",
    "    correct_time_values.attrs = ds['time'].attrs\n",
    "    ds = ds.assign_coords({\"time\": correct_time_values})\n",
    "    ds = xr.decode_cf(ds)  # decode to datetime objects\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5258fa52-bbdf-4d54-829e-c597dd77f66b",
   "metadata": {},
   "source": [
    "**Minimal pre-processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8eb78755-940d-4e6d-b536-ee343bef403f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - - - - - - - - - - - - - - - \n",
    "# Pre-process data while reading in \n",
    "# - - - - - - - - - - - - - - - \n",
    "\n",
    "def preprocess_h0(ds):\n",
    "       \n",
    "    keepVars = ['SWCF','LWCF','TS','CLOUD','FSNS','FLNS','PS','QREFHT',\n",
    "                'U10','CLDHGH','CLDLIQ','CONCLD','TMQ','P0','hyam','hybm','hyai','hybi',\n",
    "                'PHIS','USTAR','QT','GCLDLWP',\n",
    "                'THETAL','CDNUMC','CLDBOT','CLDLOW',\n",
    "                'CLDMED','CLDTOP','CLDTOT','THLP2_CLUBB','CLOUDCOVER_CLUBB','CLOUDFRAC_CLUBB',\n",
    "                'RCM_CLUBB','RTP2_CLUBB','RTPTHLP_CLUBB','RVMTEND_CLUBB','STEND_CLUBB','UP2_CLUBB','UPWP_CLUBB',\n",
    "                'VP2_CLUBB','T','Q','OMEGA','PBLH','U','V','WP2_CLUBB','WP3_CLUBB','WPRCP_CLUBB',\n",
    "                'WPRTP_CLUBB',\n",
    "                'WPTHLP_CLUBB','WPTHVP_CLUBB','Z3','PRECT','PRECC',\n",
    "                'TGCLDCWP','TGCLDLWP','GCLDLWP',\n",
    "                'LHFLX','SHFLX','TREFHT','RHREFHT']\n",
    "        \n",
    "    ds         = cesm_correct_time(ds)\n",
    "    ds['time'] = ds.indexes['time'].to_datetimeindex() \n",
    "        \n",
    "    ## Select the second simulated day for analysis \n",
    "    # iTimeStart_day2  = np.where( (ds.time.values >= (ds.time.values[0] + np.timedelta64(1,'D'))) & \n",
    "    #                              (ds.time.values < (ds.time.values[0] + np.timedelta64(2,'D'))))[0]\n",
    "    dsSel      = ds.isel(time=np.sort((ds.time.values >= (ds.time.values[0] + np.timedelta64(1,'D'))) & \n",
    "                                      (ds.time.values < (ds.time.values[0] + np.timedelta64(2,'D')))))[keepVars]\n",
    "    \n",
    "    # Compute local time \n",
    "    localTimes = dsSel['time'].values - np.timedelta64(5,'h')\n",
    "    dsSel      = dsSel.assign_coords({\"time\": localTimes})\n",
    "    \n",
    "    return dsSel\n",
    "\n",
    "def preprocess_h1(ds):\n",
    "    \n",
    "    keepVars = [\n",
    "                'wpthlp','wprtp','rtp2',\n",
    "                'thlm','rtm','wm_zm','rtm_zm','thlm_zm',\n",
    "                ]\n",
    "        \n",
    "    ds         = cesm_correct_time(ds)\n",
    "    ds['time'] = ds.indexes['time'].to_datetimeindex() \n",
    "        \n",
    "    ## Select the second simulated day for analysis \n",
    "    dsSel      = ds.isel(time=np.sort((ds.time.values >= (ds.time.values[0] + np.timedelta64(1,'D'))) & \n",
    "                                      (ds.time.values < (ds.time.values[0] + np.timedelta64(2,'D')))))[keepVars]\n",
    "\n",
    "    \n",
    "    # Compute local time \n",
    "    localTimes = dsSel['time'].values - np.timedelta64(5,'h')\n",
    "    dsSel      = dsSel.assign_coords({\"time\": localTimes})\n",
    "        \n",
    "    return dsSel\n",
    "\n",
    "\n",
    "def preprocess_h2(ds):\n",
    "    \n",
    "    varSels = np.asarray([\n",
    "                      'edmf_upa','edmf_upw','edmf_upqt','edmf_upthl','edmf_cloudfrac','edmf_dnw',\n",
    "                      'edmf_precc','edmf_uplh',\n",
    "                      'edmf_ent','edmf_upent','edmf_updet','edmf_upbuoy',\n",
    "\n",
    "                       ])\n",
    "\n",
    "    ds         = cesm_correct_time(ds)\n",
    "    ds['time'] = ds.indexes['time'].to_datetimeindex() \n",
    "        \n",
    "    ## Select the second simulated day for analysis \n",
    "    dsSel      = ds[varSels].isel(time=np.sort((ds.time.values >= (ds.time.values[0] + np.timedelta64(1,'D'))) & \n",
    "                                               (ds.time.values <= (ds.time.values[0] + np.timedelta64(2,'D')))))\n",
    "    \n",
    "    # Compute local time \n",
    "    localTimes = dsSel['time'].values - np.timedelta64(5,'h')\n",
    "    dsSel      = dsSel.assign_coords({\"time\": localTimes})\n",
    "\n",
    "    ## Replacing 'missing' updraft values with NaN \n",
    "    dsSel = dsSel.where(dsSel['edmf_upthl'] != 0.0).load()\n",
    "\n",
    "    return dsSel\n",
    "\n",
    "\n",
    "def preprocess_h2_2d(ds):\n",
    "    \n",
    "    varSels = np.asarray([\n",
    "                      'edmf_cloudfrac','edmf_qtflxup','edmf_thlflxup','edmf_precc',\n",
    "                      'edmf_S_ATHLTHL','edmf_S_AQTQT','edmf_S_AWW', 'edmf_L0',\n",
    "                       ])\n",
    "\n",
    "    ds         = cesm_correct_time(ds)\n",
    "    ds['time'] = ds.indexes['time'].to_datetimeindex() \n",
    "        \n",
    "    ## Select the second simulated day for analysis \n",
    "    dsSel      = ds[varSels].isel(time=np.sort((ds.time.values >= (ds.time.values[0] + np.timedelta64(1,'D'))) & \n",
    "                                 (ds.time.values < (ds.time.values[0] + np.timedelta64(2,'D')))))\n",
    "    \n",
    "    # Compute local time \n",
    "    localTimes = dsSel['time'].values - np.timedelta64(5,'h')\n",
    "    dsSel      = dsSel.assign_coords({\"time\": localTimes}).load()    \n",
    "    return dsSel\n",
    "\n",
    "def preprocess_h3(ds):\n",
    "\n",
    "    ds         = cesm_correct_time(ds)\n",
    "    ds['time'] = ds.indexes['time'].to_datetimeindex() \n",
    "        \n",
    "    ## Select the second simulated day for analysis \n",
    "    dsSel      = ds.isel(time=np.sort((ds.time.values >= (ds.time.values[0] + np.timedelta64(1,'D'))) & \n",
    "                                 (ds.time.values < (ds.time.values[0] + np.timedelta64(2,'D')))))\n",
    "    \n",
    "    # Compute local time \n",
    "    localTimes = dsSel['time'].values - np.timedelta64(5,'h')\n",
    "    dsSel      = dsSel.assign_coords({\"time\": localTimes}).load()\n",
    "    \n",
    "    ## Replacing 'missing' updraft values with NaN \n",
    "    dsSel   = dsSel.where(dsSel['thlu_macmic2'] != 0.0)\n",
    "    \n",
    "    return dsSel\n",
    "\n",
    "def preprocessCLM_h0(ds):\n",
    "    keepVars_CLM = ['SOILWATER_10CM','TSOI_10CM','RAIN','FSA','TG','TSA',\n",
    "                    'QVEGT','QVEGE','QSOIL','H2OSOI','TSOI','SOILLIQ']\n",
    "\n",
    "    ds['time'] = ds.indexes['time'].to_datetimeindex() \n",
    "        \n",
    "    ## Select the second simulated day for analysis \n",
    "    dsSel      = ds.isel(time=np.sort((ds.time.values >= (ds.time.values[0] + np.timedelta64(1,'D'))) & \n",
    "                                 (ds.time.values < (ds.time.values[0] + np.timedelta64(2,'D')))))[keepVars_CLM]\n",
    "    \n",
    "    # Compute local time \n",
    "    localTimes = dsSel['time'].values - np.timedelta64(5,'h')\n",
    "    dsSel      = dsSel.assign_coords({\"time\": localTimes})\n",
    "\n",
    "    return dsSel\n",
    "\n",
    "def preprocessCLM_h1(ds):\n",
    "    \n",
    "    ds['time'] = ds.indexes['time'].to_datetimeindex() \n",
    "        \n",
    "    ## Select the second simulated day for analysis \n",
    "    dsSel      = ds.isel(time=np.sort((ds.time.values >= (ds.time.values[0] + np.timedelta64(1,'D'))) & \n",
    "                                 (ds.time.values < (ds.time.values[0] + np.timedelta64(2,'D')))))\n",
    "    \n",
    "    # Compute local time \n",
    "    localTimes = dsSel['time'].values - np.timedelta64(5,'h')\n",
    "    dsSel      = dsSel.assign_coords({\"time\": localTimes})\n",
    "        \n",
    "    QFLX = dsSel.QSOIL+dsSel.QVEGE+dsSel.QVEGT\n",
    "    dsSel['QFLX'] = (('time','pft'), QFLX.values)\n",
    "    \n",
    "    return dsSel\n",
    "\n",
    "def preprocessCLM_h2(ds):\n",
    "    ds['time'] = ds.indexes['time'].to_datetimeindex() \n",
    "        \n",
    "    ## Select the second simulated day for analysis \n",
    "    dsSel      = ds.isel(time=np.sort((ds.time.values >= (ds.time.values[0] + np.timedelta64(1,'D'))) & \n",
    "                                 (ds.time.values < (ds.time.values[0] + np.timedelta64(2,'D')))))\n",
    "    \n",
    "    # Compute local time \n",
    "    localTimes = dsSel['time'].values - np.timedelta64(5,'h')\n",
    "    dsSel      = dsSel.assign_coords({\"time\": localTimes})\n",
    "        \n",
    "    QFLX = dsSel.QSOIL+dsSel.QVEGE+dsSel.QVEGT\n",
    "    dsSel['QFLX'] = (('time','landunit'), QFLX.values)\n",
    "    \n",
    "    return dsSel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2680f7-d2a6-4816-b087-110a5786783c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3e11e6-0a94-4676-afd3-2feb1da8a02c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7a2382c3-b673-42c8-9b13-ef5f86f45d1d",
   "metadata": {},
   "source": [
    "## Start up dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5fcd27ff-f1ca-4f19-91ae-8e9d3b2d60fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "\n",
    "from dask_jobqueue import PBSCluster\n",
    "\n",
    "# For Casper\n",
    "cluster = PBSCluster(\n",
    "    queue=\"casper\",\n",
    "    walltime=\"01:00:00\",\n",
    "    project=\"P93300642\",\n",
    "    #memory=\"4GB\",\n",
    "    #resource_spec=\"select=1:ncpus=1:mem=4GB\",\n",
    "    memory=\"10GB\",\n",
    "    resource_spec=\"select=1:ncpus=1:mem=10GB\",\n",
    "    cores=1,\n",
    "    processes=1,\n",
    ")\n",
    "\n",
    "# # scale as needed\n",
    "# cluster.adapt(minimum_jobs=1, maximum_jobs=30)\n",
    "# cluster\n",
    "\n",
    "# # Scale up\n",
    "# cluster.scale(8)\n",
    "# cluster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3314b34f-3617-4cf6-b887-c0af2d9e4eba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "    <div style=\"width: 24px; height: 24px; background-color: #e1e1e1; border: 3px solid #9D9D9D; border-radius: 5px; position: absolute;\"> </div>\n",
       "    <div style=\"margin-left: 48px;\">\n",
       "        <h3 style=\"margin-bottom: 0px;\">Client</h3>\n",
       "        <p style=\"color: #9D9D9D; margin-bottom: 0px;\">Client-0e3e581c-d4de-11ee-b596-3cecef1acaa4</p>\n",
       "        <table style=\"width: 100%; text-align: left;\">\n",
       "\n",
       "        <tr>\n",
       "        \n",
       "            <td style=\"text-align: left;\"><strong>Connection method:</strong> Cluster object</td>\n",
       "            <td style=\"text-align: left;\"><strong>Cluster type:</strong> dask_jobqueue.PBSCluster</td>\n",
       "        \n",
       "        </tr>\n",
       "\n",
       "        \n",
       "            <tr>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Dashboard: </strong> <a href=\"https://jupyterhub.hpc.ucar.edu/stable/user/mdfowler/Extra/proxy/8787/status\" target=\"_blank\">https://jupyterhub.hpc.ucar.edu/stable/user/mdfowler/Extra/proxy/8787/status</a>\n",
       "                </td>\n",
       "                <td style=\"text-align: left;\"></td>\n",
       "            </tr>\n",
       "        \n",
       "\n",
       "        </table>\n",
       "\n",
       "        \n",
       "            <button style=\"margin-bottom: 12px;\" data-commandlinker-command=\"dask:populate-and-launch-layout\" data-commandlinker-args='{\"url\": \"https://jupyterhub.hpc.ucar.edu/stable/user/mdfowler/Extra/proxy/8787/status\" }'>\n",
       "                Launch dashboard in JupyterLab\n",
       "            </button>\n",
       "        \n",
       "\n",
       "        \n",
       "            <details>\n",
       "            <summary style=\"margin-bottom: 20px;\"><h3 style=\"display: inline;\">Cluster Info</h3></summary>\n",
       "            <div class=\"jp-RenderedHTMLCommon jp-RenderedHTML jp-mod-trusted jp-OutputArea-output\">\n",
       "    <div style=\"width: 24px; height: 24px; background-color: #e1e1e1; border: 3px solid #9D9D9D; border-radius: 5px; position: absolute;\">\n",
       "    </div>\n",
       "    <div style=\"margin-left: 48px;\">\n",
       "        <h3 style=\"margin-bottom: 0px; margin-top: 0px;\">PBSCluster</h3>\n",
       "        <p style=\"color: #9D9D9D; margin-bottom: 0px;\">58adf985</p>\n",
       "        <table style=\"width: 100%; text-align: left;\">\n",
       "            <tr>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Dashboard:</strong> <a href=\"https://jupyterhub.hpc.ucar.edu/stable/user/mdfowler/Extra/proxy/8787/status\" target=\"_blank\">https://jupyterhub.hpc.ucar.edu/stable/user/mdfowler/Extra/proxy/8787/status</a>\n",
       "                </td>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Workers:</strong> 0\n",
       "                </td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Total threads:</strong> 0\n",
       "                </td>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Total memory:</strong> 0 B\n",
       "                </td>\n",
       "            </tr>\n",
       "            \n",
       "        </table>\n",
       "\n",
       "        <details>\n",
       "            <summary style=\"margin-bottom: 20px;\">\n",
       "                <h3 style=\"display: inline;\">Scheduler Info</h3>\n",
       "            </summary>\n",
       "\n",
       "            <div style=\"\">\n",
       "    <div>\n",
       "        <div style=\"width: 24px; height: 24px; background-color: #FFF7E5; border: 3px solid #FF6132; border-radius: 5px; position: absolute;\"> </div>\n",
       "        <div style=\"margin-left: 48px;\">\n",
       "            <h3 style=\"margin-bottom: 0px;\">Scheduler</h3>\n",
       "            <p style=\"color: #9D9D9D; margin-bottom: 0px;\">Scheduler-4fa38809-1083-4303-a0c5-67ec43dc1d62</p>\n",
       "            <table style=\"width: 100%; text-align: left;\">\n",
       "                <tr>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Comm:</strong> tcp://128.117.208.76:35697\n",
       "                    </td>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Workers:</strong> 0\n",
       "                    </td>\n",
       "                </tr>\n",
       "                <tr>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Dashboard:</strong> <a href=\"https://jupyterhub.hpc.ucar.edu/stable/user/mdfowler/Extra/proxy/8787/status\" target=\"_blank\">https://jupyterhub.hpc.ucar.edu/stable/user/mdfowler/Extra/proxy/8787/status</a>\n",
       "                    </td>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Total threads:</strong> 0\n",
       "                    </td>\n",
       "                </tr>\n",
       "                <tr>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Started:</strong> Just now\n",
       "                    </td>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Total memory:</strong> 0 B\n",
       "                    </td>\n",
       "                </tr>\n",
       "            </table>\n",
       "        </div>\n",
       "    </div>\n",
       "\n",
       "    <details style=\"margin-left: 48px;\">\n",
       "        <summary style=\"margin-bottom: 20px;\">\n",
       "            <h3 style=\"display: inline;\">Workers</h3>\n",
       "        </summary>\n",
       "\n",
       "        \n",
       "\n",
       "    </details>\n",
       "</div>\n",
       "\n",
       "        </details>\n",
       "    </div>\n",
       "</div>\n",
       "            </details>\n",
       "        \n",
       "\n",
       "    </div>\n",
       "</div>"
      ],
      "text/plain": [
       "<Client: 'tcp://128.117.208.76:35697' processes=0 threads=0, memory=0 B>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "# Connect client to the remote dask workers\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d09d338-3346-4b4c-b40a-ce60f38fb0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.scale(24)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8c47753-e0eb-452a-8a0a-af6d87e640ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "client.wait_for_workers(24)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22816dee-a809-4882-948f-0c7bfe3c3261",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'running'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b51e6778-617b-420e-978d-2bd381bda48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# client.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61699021-6e99-49cc-9a29-59e965c6f267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !qstat -u $USER\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbaae01-7b61-4a8b-b6f4-5f5cd22d5302",
   "metadata": {},
   "source": [
    "## Read in data, but don't do a ton of extra processing yet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e589f269-0303-4d05-aca9-94d340926856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Starting on case clubbMF_L0eq250_diffOff ***\n",
      "h0 files loaded\n",
      "h1 files loaded\n",
      "h2 files loaded with mfdataset\n",
      "h2 2D data loaded with mfdataset\n",
      "h3 files loaded with mfdataset\n",
      "DS merged and loaded\n",
      "CLM_h1 files loaded with mfdataset\n",
      "CLM_h2 files loaded with mfdataset\n",
      "*** Starting on case claspMF_L0eq250_diffOff ***\n",
      "h0 files loaded\n",
      "h1 files loaded\n",
      "h2 files loaded with mfdataset\n",
      "h2 2D data loaded with mfdataset\n",
      "h3 files loaded with mfdataset\n",
      "DS merged and loaded\n",
      "CLM_h1 files loaded with mfdataset\n",
      "CLM_h2 files loaded with mfdataset\n",
      "CPU times: user 8min 1s, sys: 5min 29s, total: 13min 31s\n",
      "Wall time: 19min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "testDir     = '/glade/derecho/scratch/mdfowler/CLASP+MF_diffusionOff/'\n",
    "\n",
    "case_names  = [\n",
    "                'clubbMF_L0eq250_diffOff',\n",
    "                'claspMF_L0eq250_diffOff',\n",
    "              ]\n",
    "\n",
    "caseStart = 'FSCAM.T42_T42.arm97.CLASP_CLUBBMF_'\n",
    "\n",
    "caseStrings = [\n",
    "    'usePatchDataTRUE_25each_L0eq250_nup25_newHETterm_simplExtraPlume_diffOff_allJJA.',\n",
    "    'usePatchDataTRUE_25each_L0eq250_nup25_newHETterm_simplExtraPlume_diffOff_allJJA.'\n",
    "]\n",
    "\n",
    "nens=25\n",
    "\n",
    "for iCase in range(len(case_names)):\n",
    "    print('*** Starting on case %s ***' % (case_names[iCase]))\n",
    "\n",
    "    ## Get list of files \n",
    "    listFiles_h0 = np.sort(glob.glob(testDir+caseStart+caseStrings[iCase]+'*cam.h0.2016*'))\n",
    "    listFiles_h1 = np.sort(glob.glob(testDir+caseStart+caseStrings[iCase]+'*cam.h1.2016*'))\n",
    "    listFiles_h2 = np.sort(glob.glob(testDir+caseStart+caseStrings[iCase]+'*cam.h2.2016*'))\n",
    "    listFiles_h3 = np.sort(glob.glob(testDir+caseStart+caseStrings[iCase]+'*cam.h3.2016*'))\n",
    "\n",
    "    listFilesCLM_h2 = np.sort(glob.glob(testDir+caseStart+caseStrings[iCase]+'*clm2.h2.2016*'))\n",
    "    listFilesCLM_h1 = np.sort(glob.glob(testDir+caseStart+caseStrings[iCase]+'*clm2.h1.2016*'))\n",
    "    listFilesCLM_h0 = np.sort(glob.glob(testDir+caseStart+caseStrings[iCase]+'*clm2.h0.2016*'))        \n",
    "\n",
    "    case_h0 = xr.open_mfdataset(listFiles_h0,  preprocess=preprocess_h0, concat_dim='time', \n",
    "                                combine='nested', decode_times=False, \n",
    "                                data_vars='minimal', parallel='True')\n",
    "    print('h0 files loaded')\n",
    "    case_h1 = xr.open_mfdataset(listFiles_h1,  preprocess=preprocess_h1, concat_dim='time', \n",
    "                                combine='nested', decode_times=False, \n",
    "                                data_vars='minimal', parallel='True')\n",
    "    print('h1 files loaded')\n",
    "\n",
    "    case_h2 = xr.open_mfdataset(listFiles_h2,  preprocess=preprocess_h2, concat_dim='time', \n",
    "                                combine='nested', decode_times=False, \n",
    "                                data_vars='minimal', parallel='True')\n",
    "    print('h2 files loaded with mfdataset')\n",
    "    case_h2_2d = xr.open_mfdataset(listFiles_h2,  preprocess=preprocess_h2_2d, concat_dim='time', \n",
    "                                combine='nested', decode_times=False, \n",
    "                                data_vars='minimal', parallel='True')\n",
    "    print('h2 2D data loaded with mfdataset')\n",
    "    \n",
    "    case_h3 = xr.open_mfdataset(listFiles_h3,  preprocess=preprocess_h3, concat_dim='time', \n",
    "                                combine='nested', decode_times=False, \n",
    "                                data_vars='minimal', parallel='True')\n",
    "    print('h3 files loaded with mfdataset')\n",
    "        \n",
    "    # Merge cases and load\n",
    "    caseFull = xr.merge([case_h1, case_h0])\n",
    "    del case_h0,case_h1\n",
    "    \n",
    "    caseH2   = xr.merge([case_h2, case_h2_2d])\n",
    "    caseH2   = caseH2.assign_coords({\"nens\": np.arange(nens)})\n",
    "    del case_h2,case_h2_2d\n",
    "\n",
    "    if iCase==0:\n",
    "        ncyc = len(case_h3.ncyc.values)\n",
    "    \n",
    "    caseH3   = case_h3.assign_coords({\"nens\": np.arange(nens)})\n",
    "    caseH3   = caseH3.assign_coords({\"ncyc\": np.arange(ncyc)})\n",
    "    del case_h3    \n",
    "    # del case_h0,case_h1,case_h2,case_h2_2d,case_h3\n",
    "\n",
    "    print('DS merged and loaded')\n",
    "\n",
    "    ## Label with case info \n",
    "    caseFull_allDays   = caseFull.squeeze().assign_coords({\"case\":  case_names[iCase]})\n",
    "    caseH2_allDays     = caseH2.squeeze().assign_coords({\"case\":  case_names[iCase]})\n",
    "    caseH3_allDays     = caseH3.squeeze().assign_coords({\"case\":  case_names[iCase]})\n",
    "\n",
    "\n",
    "    ## Read in CLM datasets too\n",
    "    # - - - - - - - - - - - - - - - - - - - - \n",
    "    # caseCLM_h0 = xr.open_mfdataset(listFilesCLM_h0,  preprocess=preprocessCLM_h0, concat_dim='time', \n",
    "    #                             combine='nested', decode_times=True, \n",
    "    #                             data_vars='minimal', parallel='True')\n",
    "    # print('CLM_h0 files loaded with mfdataset')\n",
    "\n",
    "    caseCLM_h1 = xr.open_mfdataset(listFilesCLM_h1,  preprocess=preprocessCLM_h1, concat_dim='time', \n",
    "                                combine='nested', decode_times=True, \n",
    "                                data_vars='minimal', parallel='True')\n",
    "    print('CLM_h1 files loaded with mfdataset')\n",
    "    \n",
    "    caseCLM_h2 = xr.open_mfdataset(listFilesCLM_h2,  preprocess=preprocessCLM_h2, concat_dim='time', \n",
    "                                combine='nested', decode_times=True, \n",
    "                                data_vars='minimal', parallel='True')\n",
    "    print('CLM_h2 files loaded with mfdataset')\n",
    "\n",
    "    caseCLMh1_allDays = caseCLM_h1.assign_coords({\"case\":  case_names[iCase]})\n",
    "    caseCLMh2_allDays = caseCLM_h2.assign_coords({\"case\":  case_names[iCase]})\n",
    "\n",
    "    del caseCLM_h1,caseCLM_h2\n",
    "\n",
    "    ## Combine everthing into one larger array \n",
    "    # - - - - - - - - - - - - - - - - - - - - \n",
    "    if iCase==0:\n",
    "        scamDS    = caseFull_allDays\n",
    "        del caseFull_allDays\n",
    "        scamDS_h2 = caseH2_allDays\n",
    "        del caseH2_allDays\n",
    "        scamDS_h3 = caseH3_allDays\n",
    "        del caseH3_allDays\n",
    "        \n",
    "        clmDS_h1 = caseCLMh1_allDays\n",
    "        del caseCLMh1_allDays\n",
    "        clmDS_h2 = caseCLMh2_allDays\n",
    "        del caseCLMh2_allDays\n",
    "    else: \n",
    "        scamDS    = xr.concat([scamDS, caseFull_allDays], \"case\") \n",
    "        del caseFull_allDays\n",
    "        scamDS_h2 = xr.concat([scamDS_h2, caseH2_allDays], \"case\") \n",
    "        del caseH2_allDays\n",
    "        scamDS_h3 = xr.concat([scamDS_h3, caseH3_allDays], \"case\") \n",
    "        del caseH3_allDays\n",
    "        \n",
    "        clmDS_h1 = xr.concat([clmDS_h1, caseCLMh1_allDays], \"case\") \n",
    "        del caseCLMh1_allDays\n",
    "        clmDS_h2 = xr.concat([clmDS_h2, caseCLMh2_allDays], \"case\") \n",
    "        del caseCLMh2_allDays\n",
    "         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e4f4a4-78eb-4e88-8a67-0941725f8eb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7876ae8-345c-4782-9e8f-8fc9a18816bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39564fde-14c9-4201-84e3-3a02717704b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0f005767-7fdb-4a8b-b64c-fc4acc8e0a9f",
   "metadata": {},
   "source": [
    "**Okay, now how about some *more efficient* processing?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6cb0d4-20a6-4500-b32c-61f2265e0296",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42796e5-6ab8-4934-af2b-2d6c35b6b328",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9b6c4006-6fa0-47b8-8b69-caea7e17355a",
   "metadata": {},
   "source": [
    "Test the old way first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "adcc5c2d-b7b0-4a7e-8e7d-70466e88ee74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - - - - - - - - - - - - - - - \n",
    "# Additional processing after files are read in \n",
    "# - - - - - - - - - - - - - - - \n",
    "\n",
    "def process_camData(DS):\n",
    "    \n",
    "    ## Interpolate to standard levels \n",
    "    # - - - - - - - - - - - - - - - - - - - - - - - -\n",
    "    print('Beginning interpolation...') \n",
    "    \n",
    "    # Decide on levels to interpoalte to and add to larger arrays\n",
    "    pnew64 = np.arange(200.0,980.0,10.0) \n",
    "    \n",
    "    DS = DS.assign_coords({\"levInterp\": pnew64})\n",
    "\n",
    "    varSels = np.asarray(['THLP2_CLUBB','RTP2_CLUBB','RTPTHLP_CLUBB','WPRTP_CLUBB','WPTHLP_CLUBB','WP3_CLUBB','WP2_CLUBB','UP2_CLUBB',\n",
    "                          'VP2_CLUBB','Z3','U','V','T','Q','OMEGA','RVMTEND_CLUBB','STEND_CLUBB','CLDLIQ','CLOUD','CLOUDFRAC_CLUBB',\n",
    "                          'UPWP_CLUBB','THETAL',\n",
    "                          'CONCLD','QT','GCLDLWP',\n",
    "                          'wpthlp','wprtp','rtp2',\n",
    "                          'thlm','rtm','wm_zm','rtm_zm','thlm_zm',\n",
    "                          ])\n",
    "\n",
    "    for iVar in range(len(varSels)): \n",
    "\n",
    "        DS[varSels[iVar]] = DS[varSels[iVar]].expand_dims({'lat': 1}, axis=-1)\n",
    "        DS[varSels[iVar]] = DS[varSels[iVar]].expand_dims({'lon': 1}, axis=-1)\n",
    "        \n",
    "        # Interpolate variables and add to larger arrays \n",
    "        interpVar_real = interpolateToPressure_v2(DS, varSels[iVar], pnew64)\n",
    "\n",
    "        if len(np.shape(interpVar_real))==2: \n",
    "            DS[varSels[iVar]+'_interp']  = (('time','levInterp'), interpVar_real)\n",
    "        elif len(np.shape(interpVar_real))==3: \n",
    "            DS[varSels[iVar]+'_interp']  = (('time','nens','levInterp'), interpVar_real)\n",
    "\n",
    "        \n",
    "    return DS\n",
    "\n",
    "\n",
    "def process_camData_h2(DS, DSctrl):\n",
    "    ## Interpolate to standard levels \n",
    "    # - - - - - - - - - - - - - - - - - - - - - - - -\n",
    "    print('Beginning interpolation...') \n",
    "    \n",
    "    # Decide on levels to interpoalte to and add to larger arrays\n",
    "    pnew64 = np.arange(200.0,980.0,10.0) \n",
    "    \n",
    "    DS = DS.assign_coords({\"levInterp\": pnew64})\n",
    "    varSels = np.asarray([\n",
    "                          'edmf_upa','edmf_upw','edmf_upqt','edmf_upthl',\n",
    "                          'edmf_cloudfrac','edmf_dnw','edmf_precc',\n",
    "                          'edmf_qtflxup','edmf_thlflxup',\n",
    "                          'edmf_S_ATHLTHL','edmf_S_AQTQT','edmf_S_AWW',\n",
    "                           'edmf_ent','edmf_upent','edmf_updet','edmf_upbuoy',\n",
    "\n",
    "                           ])\n",
    "\n",
    "    for iVar in range(len(varSels)): \n",
    "        DS[varSels[iVar]] = DS[varSels[iVar]].expand_dims({'lat': 1}, axis=-1)\n",
    "        DS[varSels[iVar]] = DS[varSels[iVar]].expand_dims({'lon': 1}, axis=-1)\n",
    "\n",
    "        # Interpolate variables and add to larger arrays \n",
    "        interpVar_real = interpolateToPressure_v2_h2(DS, DSctrl, varSels[iVar], pnew64)\n",
    "\n",
    "        if len(np.shape(interpVar_real))==2: \n",
    "            DS[varSels[iVar]+'_interp']  = (('time','levInterp'), interpVar_real)\n",
    "        elif len(np.shape(interpVar_real))==3: \n",
    "            DS[varSels[iVar]+'_interp']  = (('time','nens','levInterp'), interpVar_real)\n",
    "\n",
    "    return DS\n",
    "\n",
    "def process_camData_h3(DS, DSctrl):\n",
    "    \n",
    "    ## Interpolate to standard levels \n",
    "    # - - - - - - - - - - - - - - - - - - - - - - - -\n",
    "    print('Beginning interpolation...') \n",
    "    \n",
    "    # Decide on levels to interpoalte to and add to larger arrays\n",
    "    pnew64 = np.arange(200.0,980.0,10.0) \n",
    "    \n",
    "    DS = DS.assign_coords({\"levInterp\": pnew64})\n",
    "    \n",
    "    varSels = np.asarray(['up_macmicAvg', 'dn_macmicAvg','upa_macmicAvg','dna_macmicAvg',\n",
    "               'thlu_macmicAvg','qtu_macmicAvg','thld_macmicAvg','qtd_macmicAvg' ])\n",
    "    \n",
    "    for iVar in range(len(varSels)): \n",
    "        DS[varSels[iVar]] = DS[varSels[iVar]].expand_dims({'lat': 1}, axis=-1)\n",
    "        DS[varSels[iVar]] = DS[varSels[iVar]].expand_dims({'lon': 1}, axis=-1)\n",
    "\n",
    "        # Interpolate variables and add to larger arrays \n",
    "        interpVar_real = interpolateToPressure_v2_h3(DS, DSctrl, varSels[iVar], pnew64)\n",
    "\n",
    "        if len(np.shape(interpVar_real))==2: \n",
    "            DS[varSels[iVar]+'_interp']  = (('time','levInterp'), interpVar_real)\n",
    "        elif len(np.shape(interpVar_real))==3: \n",
    "            DS[varSels[iVar]+'_interp']  = (('time','nens','levInterp'), interpVar_real)\n",
    "        elif len(np.shape(interpVar_real))==4: \n",
    "            DS[varSels[iVar]+'_interp']  = (('time','ncyc','nens','levInterp'), interpVar_real)\n",
    "        \n",
    "    return DS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "86604e1c-4b9c-4d9a-93f7-721e5fbf7f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - - - - - - - - - - - - - - - \n",
    "# Pressure interpolation \n",
    "# - - - - - - - - - - - - - - - \n",
    "\n",
    "def interpolateToPressure_v2(DS, varName, pressGoals):\n",
    "    p0mb = DS.P0.values/100        # mb\n",
    "\n",
    "    # Pull out hya/hyb profiles \n",
    "    hyam = np.squeeze(DS.hyam.values)[:]\n",
    "    hybm = np.squeeze(DS.hybm.values)[:]\n",
    "    hyai = np.squeeze(DS.hyai.values)[:]\n",
    "    hybi = np.squeeze(DS.hybi.values)[:]\n",
    "\n",
    "    # Surface pressure with time dimension\n",
    "    PS   = DS.PS.values              # Pa\n",
    "\n",
    "    # Converting variables: \n",
    "    if np.shape(DS[varName].values)[1]==len(DS.ilev.values):\n",
    "        varInterp = Ngl.vinth2p(DS[varName].values,hyai,hybi,pressGoals,PS,1,p0mb,1,True)\n",
    "    elif np.shape(DS[varName].values)[1]==len(DS.lev.values):\n",
    "        varInterp = Ngl.vinth2p(DS[varName].values,hyam,hybm,pressGoals,PS,1,p0mb,1,True)\n",
    "    ## Handle data that's by-plume for EDMF output\n",
    "    elif np.shape(DS[varName].values)[1]==len(DS.nens.values):\n",
    "        varInterp = np.full([len(DS.time.values), len(DS.nens.values) ,\n",
    "                             len(pressGoals)], np.nan)\n",
    "        \n",
    "        for iEns in range(len(DS.nens.values)):\n",
    "            varInterp[:,iEns,:] = np.squeeze(Ngl.vinth2p(DS[varName].values[:,iEns,:],hyai,hybi,pressGoals,PS,1,p0mb,1,True))\n",
    "\n",
    "    saveOut = varInterp\n",
    "    \n",
    "    return saveOut\n",
    "\n",
    "def interpolateToPressure_v2_h2(DS, DSctrl, varName, pressGoals):\n",
    "    p0mb = DSctrl.P0.values/100        # mb\n",
    "\n",
    "    # Pull out hya/hyb profiles \n",
    "    hyam = np.squeeze(DSctrl.hyam.values)[:]\n",
    "    hybm = np.squeeze(DSctrl.hybm.values)[:]\n",
    "    hyai = np.squeeze(DSctrl.hyai.values)[:]\n",
    "    hybi = np.squeeze(DSctrl.hybi.values)[:]\n",
    "\n",
    "    # Surface pressure with time dimension\n",
    "    PS   = DSctrl.PS.values              # Pa \n",
    "\n",
    "    # Converting variables: \n",
    "    if np.shape(DS[varName].values)[1]==len(DS.ilev.values):\n",
    "        varInterp = Ngl.vinth2p(DS[varName].values,hyai,hybi,pressGoals,PS,1,p0mb,1,True)\n",
    "    elif np.shape(DS[varName].values)[1]==len(DS.lev.values):\n",
    "        varInterp = Ngl.vinth2p(DS[varName].values,hyam,hybm,pressGoals,PS,1,p0mb,1,True)\n",
    "    ## Handle data that's by-plume for EDMF output\n",
    "    elif np.shape(DS[varName].values)[1]==len(DS.nens.values):\n",
    "        varInterp = np.full([len(DS.time.values), len(DS.nens.values) ,\n",
    "                             len(pressGoals)], np.nan)\n",
    "        \n",
    "        for iEns in range(len(DS.nens.values)):\n",
    "            varInterp[:,iEns,:] = np.squeeze(Ngl.vinth2p(DS[varName].values[:,iEns,:],hyai,hybi,pressGoals,PS,1,p0mb,1,True))\n",
    "\n",
    "    saveOut = varInterp\n",
    "    \n",
    "    return saveOut\n",
    "\n",
    "\n",
    "def interpolateToPressure_v2_h3(DS, DSctrl, varName, pressGoals):\n",
    "    p0mb = DSctrl.P0.values/100        # mb\n",
    "\n",
    "    # Pull out hya/hyb profiles \n",
    "    hyam = np.squeeze(DSctrl.hyam.values)[:]\n",
    "    hybm = np.squeeze(DSctrl.hybm.values)[:]\n",
    "    hyai = np.squeeze(DSctrl.hyai.values)[:]\n",
    "    hybi = np.squeeze(DSctrl.hybi.values)[:]\n",
    "\n",
    "    # Surface pressure with time dimension\n",
    "    PS   = DSctrl.PS.values              # Pa \n",
    "\n",
    "    # Converting variables: \n",
    "    if np.shape(DS[varName].values)[1]==len(DS.ilev.values):\n",
    "        varInterp = Ngl.vinth2p(DS[varName].values,hyai,hybi,pressGoals,PS,1,p0mb,1,True)\n",
    "    elif np.shape(DS[varName].values)[1]==len(DS.lev.values):\n",
    "        varInterp = Ngl.vinth2p(DS[varName].values,hyam,hybm,pressGoals,PS,1,p0mb,1,True)\n",
    "    ## Handle data that's by-plume for EDMF output\n",
    "    elif np.shape(DS[varName].values)[1]==len(DS.nens.values):\n",
    "        varInterp = np.full([len(DS.time.values), len(DS.nens.values),\n",
    "                             len(pressGoals)], np.nan)\n",
    "        \n",
    "        for iEns in range(len(DS.nens.values)):\n",
    "            varInterp[:,iEns,:] = np.squeeze(Ngl.vinth2p(DS[varName].values[:,iEns,:],hyai,hybi,pressGoals,PS,1,p0mb,1,True))\n",
    " \n",
    "    ## Handle data that's by-plume *and* by subcycle for EDMF output\n",
    "    elif np.shape(DS[varName].values)[2]==len(DS.nens.values):\n",
    "        varInterp = np.full([len(DS.time.values), len(DS.ncyc.values), len(DS.nens.values) ,\n",
    "                             len(pressGoals)], np.nan)\n",
    "        \n",
    "        for iEns in range(len(DS.nens.values)):\n",
    "            for iCyc in range(len(DS.ncyc.values)):\n",
    "                varInterp[:,iCyc,iEns,:] = np.squeeze(Ngl.vinth2p(DS[varName].values[:,iCyc,iEns,:],hyai,hybi,pressGoals,PS,1,p0mb,1,True))\n",
    "\n",
    "            \n",
    "    saveOut = np.squeeze(varInterp)\n",
    "    \n",
    "    return saveOut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "69077c8f-5e0f-4d5c-a962-81724759e86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combineMacmic_beforeInterp(ds_h3):\n",
    "    var1    = ['up_macmic1', 'dn_macmic1','upa_macmic1','dna_macmic1',\n",
    "           'thlu_macmic1','qtu_macmic1','thld_macmic1','qtd_macmic1']\n",
    "\n",
    "    var2    = ['up_macmic2', 'dn_macmic2','upa_macmic2','dna_macmic2',\n",
    "               'thlu_macmic2','qtu_macmic2','thld_macmic2','qtd_macmic2']\n",
    "\n",
    "    varSave = ['up_macmicAvg', 'dn_macmicAvg','upa_macmicAvg','dna_macmicAvg',\n",
    "               'thlu_macmicAvg','qtu_macmicAvg','thld_macmicAvg','qtd_macmicAvg']\n",
    "\n",
    "    for iVar in range(len(var1)):\n",
    "#         print('Computing %s' % (varSave[iVar]))\n",
    "\n",
    "        varCyc1 = ds_h3[var1[iVar]]\n",
    "        varCyc2 = ds_h3[var2[iVar]]\n",
    "        \n",
    "        ## ADDED 8/8/23: Need to filter out areas that are zero...\n",
    "        condition1 = ds_h3['upa_macmic1']>0\n",
    "        subset_ds1 = varCyc1.where(condition1)\n",
    "\n",
    "        condition2 = ds_h3['upa_macmic2']>0\n",
    "        subset_ds2 = varCyc2.where(condition2)\n",
    "\n",
    "        ## This seems to work... \n",
    "        # s = np.stack((varCyc1, varCyc2))\n",
    "        s = np.stack((subset_ds1, subset_ds2))\n",
    "        # C = np.nansum(s, axis=0)\n",
    "        C = np.nanmean(s, axis=0)\n",
    "        C[np.all(np.isnan(s), axis=0)] = np.nan\n",
    "\n",
    "        # ds_h3[varSave[iVar]]  = (('time','nens','ilev','lat','lon'), C)\n",
    "        ds_h3[varSave[iVar]]  = (('time','nens','ilev'), np.squeeze(C))\n",
    "        \n",
    "        # ds_h3[varSave[iVar]] = ds_h3[varSave[iVar]].expand_dims({'lat': 1}, axis=-1)\n",
    "        # ds_h3[varSave[iVar]] = ds_h3[varSave[iVar]].expand_dims({'lon': 1}, axis=-1)\n",
    "        \n",
    "        if var1[iVar]!='upa_macmic1': \n",
    "            ds_h3 = ds_h3.drop_vars(var1[iVar])\n",
    "        if var2[iVar]!='upa_macmic2':\n",
    "            ds_h3 = ds_h3.drop_vars(var2[iVar])\n",
    "    \n",
    "    return ds_h3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "11353c04-85f3-4ed8-8f5a-a04927ae8cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "scamDS['PS'] = scamDS['PS'].expand_dims({'lat': 1}, axis=-1)\n",
    "scamDS['PS'] = scamDS['PS'].expand_dims({'lon': 1}, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e91d110-761b-47e0-946f-97778072b6c8",
   "metadata": {},
   "source": [
    "**Start up dask server here**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6bc574a2-1fc5-4428-b446-26f3929d63c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey look! scamDS all loaded in.\n",
      "CPU times: user 22.9 s, sys: 1.6 s, total: 24.5 s\n",
      "Wall time: 32.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "scamDS.load() \n",
    "print('Hey look! scamDS all loaded in.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d07a0ed8-c563-49db-87f6-419645774ada",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# scamDS_h2.load()\n",
    "# # scamDS_h2.compute()\n",
    "# # scamDS_h2 = client.persist(scamDS_h2)\n",
    "\n",
    "# print('scamDS_h2 made it too! All warshed up and clean')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6a9326d2-d615-4fff-b140-412d38df7b11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning interpolation...\n",
      "Beginning interpolation...\n",
      "Done with procDS\n",
      "Beginning interpolation...\n",
      "Beginning interpolation...\n",
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/glade/u/apps/opt/conda/envs/npl-2024a/lib/python3.11/site-packages/IPython/core/magics/execution.py\", line 1340, in time\n",
      "    exec(code, glob, local_ns)\n",
      "  File \"<timed exec>\", line 12, in <module>\n",
      "  File \"/glade/u/apps/opt/conda/envs/npl-2024a/lib/python3.11/site-packages/xarray/core/dataset.py\", line 5877, in drop_vars\n",
      "    self._assert_all_in_dataset(names_set)\n",
      "  File \"/glade/u/apps/opt/conda/envs/npl-2024a/lib/python3.11/site-packages/xarray/core/dataset.py\", line 5741, in _assert_all_in_dataset\n",
      "    raise ValueError(\n",
      "ValueError: These variables cannot be found in this dataset: ['time_written', 'sol_tsi', 'nbsec', 'hyam', 'gw', 'ndbase', 'thlu_macmicAvg', 'thld_macmicAvg', 'thld_macmicAvg_interp', 'qtd_macmicAvg', 'mdt', 'nscur', 'nbdate', 'P0', 'date', 'date_written', 'nsteph', 'hybm', 'nsbase', 'f11vmr', 'ndcur', 'dn_macmicAvg', 'time_bnds', 'n2ovmr', 'up_macmicAvg', 'qtd_macmicAvg_interp', 'upa_macmicAvg', 'hyai', 'co2vmr', 'upa_macmic2', 'ntrm', 'ntrn', 'ntrk', 'ch4vmr', 'f12vmr', 'qtu_macmicAvg', 'hybi', 'upa_macmic1', 'dna_macmicAvg', 'datesec']\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/glade/u/apps/opt/conda/envs/npl-2024a/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 2144, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/glade/u/apps/opt/conda/envs/npl-2024a/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1435, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/glade/u/apps/opt/conda/envs/npl-2024a/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1326, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/glade/u/apps/opt/conda/envs/npl-2024a/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1173, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/glade/u/apps/opt/conda/envs/npl-2024a/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1063, in format_exception_as_a_whole\n",
      "    self.get_records(etb, number_of_lines_of_context, tb_offset) if etb else []\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/glade/u/apps/opt/conda/envs/npl-2024a/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1155, in get_records\n",
      "    FrameInfo(\n",
      "  File \"/glade/u/apps/opt/conda/envs/npl-2024a/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 780, in __init__\n",
      "    ix = inspect.getsourcelines(frame)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/glade/u/apps/opt/conda/envs/npl-2024a/lib/python3.11/inspect.py\", line 1244, in getsourcelines\n",
      "    lines, lnum = findsource(object)\n",
      "                  ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/glade/u/apps/opt/conda/envs/npl-2024a/lib/python3.11/inspect.py\", line 1081, in findsource\n",
      "    raise OSError('could not get source code')\n",
      "OSError: could not get source code\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "## Process data \n",
    "procDS_case0    = process_camData( scamDS.isel(case=0) )\n",
    "procDS_case1    = process_camData( scamDS.isel(case=1) )\n",
    "procDS = xr.concat([procDS_case0, procDS_case1], \"case\") \n",
    "print('Done with procDS')\n",
    "#del scamDS\n",
    "\n",
    "procDS_h2_case0 = process_camData_h2( scamDS_h2.isel(case=0) , procDS.isel(case=0)  )\n",
    "procDS_h2_case1 = process_camData_h2( scamDS_h2.isel(case=1) , procDS.isel(case=1)  )\n",
    "procDS_h2 = xr.concat([procDS_h2_case0, procDS_h2_case1], \"case\") \n",
    "## Drop excessive variables to save space\n",
    "procDS_h2 = procDS_h2.drop_vars(['ntrk','ntrn','ntrm','gw','hyam','hybm',                                  \n",
    "                             'P0','hyai','hybi','date','datesec','time_bnds','date_written',\n",
    "                             'time_written','ndbase','nsbase','nbdate','nbsec','mdt','ndcur',\n",
    "                             'nscur','co2vmr','ch4vmr','n2ovmr','f11vmr','f12vmr','sol_tsi','nsteph',\n",
    "                              ## Also remove downdraft data (not active here)\n",
    "                             'qtd_macmicAvg_interp','thld_macmicAvg_interp',\n",
    "                              'qtd_macmicAvg','thld_macmicAvg','dna_macmicAvg',\n",
    "                              'dn_macmicAvg','upa_macmic2','upa_macmic1','edmf_dnw_interp',\n",
    "                              # 'dn_macmicAvg_interp', 'dna_macmicAvg_interp',  \n",
    "                              ## Also drop macmicAvg if not interpolated \n",
    "                              'up_macmicAvg','upa_macmicAvg','thlu_macmicAvg','qtu_macmicAvg',\n",
    "                              ## Riskier - but drop the things that are just the last time-step\n",
    "                              # 'edmf_upa','edmf_upw','edmf_upqt','edmf_upthl',\n",
    "                                ])\n",
    "print('Done with procDS_h2')\n",
    "#del scamDS_h2\n",
    "\n",
    "procDS_h3_combine_case0 = combineMacmic_beforeInterp(scamDS_h3.isel(case=0))\n",
    "procDS_h3_combine_case1 = combineMacmic_beforeInterp(scamDS_h3.isel(case=1))\n",
    "procDS_h3_combine = xr.concat([procDS_h3_combine_case0, procDS_h3_combine_case1], \"case\")\n",
    "# procDS_h3_combine.load() \n",
    "# print('We loaded h3!')\n",
    "# del scamDS_h3\n",
    "procDS_h3_case0 = process_camData_h3( procDS_h3_combine.isel(case=0), procDS.isel(case=0)  )\n",
    "procDS_h3_case1 = process_camData_h3( procDS_h3_combine.isel(case=1), procDS.isel(case=1)  )\n",
    "procDS_h3 = xr.concat([procDS_h3_case0, procDS_h3_case1], \"case\")\n",
    "del procDS_h3_combine\n",
    "print('Done with procDS_h3')\n",
    "\n",
    "procDS_h2 = xr.merge([procDS_h2, procDS_h3])\n",
    "del procDS_h3\n",
    "        \n",
    "## Combine all the cases into \n",
    "case_allDays      = procDS\n",
    "h2_allDays        = procDS_h2\n",
    "del procDS_h2, procDS\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47700471-fb88-44ae-88c7-b557a07336aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ff0c6a23-edb0-4be9-85a2-146f206013b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500ef65f-9866-4988-a240-c0edc26bbed8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e0551d-7475-461e-906a-f2c5ec6014fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18d1c71-20c3-447f-8387-892403dc3dec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bea04997-dade-42ab-928c-9500a4d264be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DSctrl = procDS.isel(case=0)\n",
    "DS = ds1\n",
    "varName = 'up_macmicAvg'\n",
    "pressGoals = np.arange(200.0,980.0,10.0) \n",
    "\n",
    "\n",
    "\n",
    "p0mb = DSctrl.P0.values/100        # mb\n",
    "\n",
    "# Pull out hya/hyb profiles \n",
    "hyam = np.squeeze(DSctrl.hyam.values)[:]\n",
    "hybm = np.squeeze(DSctrl.hybm.values)[:]\n",
    "hyai = np.squeeze(DSctrl.hyai.values)[:]\n",
    "hybi = np.squeeze(DSctrl.hybi.values)[:]\n",
    "\n",
    "# Surface pressure with time dimension\n",
    "PS   = DSctrl.PS.values              # Pa \n",
    "\n",
    "# Converting variables: \n",
    "if np.shape(DS[varName].values)[1]==len(DS.ilev.values):\n",
    "    varInterp = Ngl.vinth2p(DS[varName].values,hyai,hybi,pressGoals,PS,1,p0mb,1,True)\n",
    "elif np.shape(DS[varName].values)[1]==len(DS.lev.values):\n",
    "    varInterp = Ngl.vinth2p(DS[varName].values,hyam,hybm,pressGoals,PS,1,p0mb,1,True)\n",
    "## Handle data that's by-plume for EDMF output\n",
    "elif np.shape(DS[varName].values)[1]==len(DS.nens.values):\n",
    "    varInterp = np.full([len(DS.time.values), len(DS.nens.values),\n",
    "                         len(pressGoals)], np.nan)\n",
    "    \n",
    "    for iEns in range(len(DS.nens.values)):\n",
    "        varInterp[:,iEns,:] = Ngl.vinth2p(DS[varName].values[:,iEns,:],hyai,hybi,pressGoals,PS,1,p0mb,1,True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "946574b9-d563-47d7-b101-ca89afff9a37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(DS.nens.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74964b5-b1de-4bb2-a50f-c7392df45c06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ec45eb-1e9b-4604-bf9c-47dc9d293557",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341dd5ee-7875-4c2a-81f5-d9de98f145fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4296418-eb10-41f8-83e6-ab8fd1876ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_camData_h3(DS, DSctrl):\n",
    "    \n",
    "    ## Interpolate to standard levels \n",
    "    # - - - - - - - - - - - - - - - - - - - - - - - -\n",
    "    print('Beginning interpolation...') \n",
    "    \n",
    "    # Decide on levels to interpoalte to and add to larger arrays\n",
    "    pnew64 = np.arange(200.0,980.0,10.0) \n",
    "    \n",
    "    DS = DS.assign_coords({\"levInterp\": pnew64})\n",
    "    \n",
    "    varSels = np.asarray(['up_macmicAvg', 'dn_macmicAvg','upa_macmicAvg','dna_macmicAvg',\n",
    "               'thlu_macmicAvg','qtu_macmicAvg','thld_macmicAvg','qtd_macmicAvg' ])\n",
    "    \n",
    "    for iVar in range(len(varSels)): \n",
    "        # Interpolate variables and add to larger arrays \n",
    "        interpVar_real = interpolateToPressure_v2_h3(DS, DSctrl, varSels[iVar], pnew64)\n",
    "\n",
    "        if len(np.shape(interpVar_real))==2: \n",
    "            DS[varSels[iVar]+'_interp']  = (('time','levInterp'), interpVar_real)\n",
    "        elif len(np.shape(interpVar_real))==3: \n",
    "            DS[varSels[iVar]+'_interp']  = (('time','nens','levInterp'), interpVar_real)\n",
    "        elif len(np.shape(interpVar_real))==4: \n",
    "            DS[varSels[iVar]+'_interp']  = (('time','ncyc','nens','levInterp'), interpVar_real)\n",
    "        \n",
    "    return DS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4fc2dc-8a73-42a7-ad60-f9d95d3c86c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def interpolateToPressure_v2_h3(DS, DSctrl, varName, pressGoals):\n",
    "    p0mb = DSctrl.P0.values/100        # mb\n",
    "\n",
    "    # Pull out hya/hyb profiles \n",
    "    hyam = np.squeeze(DSctrl.hyam.values)[:]\n",
    "    hybm = np.squeeze(DSctrl.hybm.values)[:]\n",
    "    hyai = np.squeeze(DSctrl.hyai.values)[:]\n",
    "    hybi = np.squeeze(DSctrl.hybi.values)[:]\n",
    "\n",
    "    # Surface pressure with time dimension\n",
    "    PS   = DSctrl.PS.values              # Pa \n",
    "\n",
    "    # Converting variables: \n",
    "    if np.shape(DS[varName].values)[1]==len(DS.ilev.values):\n",
    "        varInterp = Ngl.vinth2p(DS[varName].values,hyai,hybi,pressGoals,PS,1,p0mb,1,True)\n",
    "    elif np.shape(DS[varName].values)[1]==len(DS.lev.values):\n",
    "        varInterp = Ngl.vinth2p(DS[varName].values,hyam,hybm,pressGoals,PS,1,p0mb,1,True)\n",
    "    ## Handle data that's by-plume for EDMF output\n",
    "    elif np.shape(DS[varName].values)[1]==len(DS.nens.values):\n",
    "        varInterp = np.full([len(DS.time.values), len(DS.nens.values),\n",
    "                             len(pressGoals)], np.nan)\n",
    "        \n",
    "        for iEns in range(len(DS.nens.values)):\n",
    "            varInterp[:,iEns,:] = Ngl.vinth2p(DS[varName].values[:,iEns,:,],hyai,hybi,pressGoals,PS,1,p0mb,1,True)\n",
    " \n",
    "    ## Handle data that's by-plume *and* by subcycle for EDMF output\n",
    "    elif np.shape(DS[varName].values)[2]==len(DS.nens.values):\n",
    "        varInterp = np.full([len(DS.time.values), len(DS.ncyc.values), len(DS.nens.values) ,\n",
    "                             len(pressGoals)], np.nan)\n",
    "        \n",
    "        for iEns in range(len(DS.nens.values)):\n",
    "            for iCyc in range(len(DS.ncyc.values)):\n",
    "                varInterp[:,iCyc,iEns,:] = Ngl.vinth2p(DS[varName].values[:,iCyc,iEns,:],hyai,hybi,pressGoals,PS,1,p0mb,1,True)\n",
    "\n",
    "            \n",
    "    saveOut = varInterp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96dcb149-c5dd-49e0-816b-98fb806c92f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc8077d-4e84-40f1-a850-5e319e948579",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a25e5ab-d008-42af-92ac-175781e508ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae810b0-996a-483a-9129-9320ea4ae36b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f86b5a-a69a-4aa4-8f63-be7bfe6fc6a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7416eb60-8929-4ccd-b65e-5a0d1823591f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0716c82c-fdd5-419a-8c99-2a4c06d4e38f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264cd00c-15bb-455e-9d09-da8fe0ab0502",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c84d307-7e96-4546-9c3b-3c7ff286be30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e14717-cc17-4fd2-8ef5-833f3ee0e256",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b64720-15b3-4ba9-81fd-e1dadd757da3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ce7872-0d9b-47eb-b4fa-cfde739b3754",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1988eb8e-b32c-4aa4-879f-eab8b3af87d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%% time\n",
    "plumeLabel = np.full([len(scamDS_h2.case.values), len(scamDS_h2.time.values), len(scamDS_h2.nens.values)], 'SurfaceAvg')\n",
    "\n",
    "sigDig     = 9\n",
    "\n",
    "for iCase in range(len(clmDS_h2.case.values)):\n",
    "\n",
    "    for iT in range(len(clmDS_h1.time.values)-1):\n",
    "\n",
    "        # Sel time QFLX and edmf_uplh \n",
    "        this_uplh    = scamDS_h2.isel(case=(iCase)).isel(time=iT, ilev=-1).edmf_uplh.values\n",
    "        this_qflx    = clmDS_h1.isel(case=(iCase)).isel(time=iT).QFLX \n",
    "        this_qflx_LU = clmDS_h2.isel(case=iCase).isel(time=iT).QFLX \n",
    "        this_upa     = scamDS_h2.isel(case=(iCase)).isel(time=iT, ilev=-1).edmf_upa.values\n",
    "\n",
    "        iGrass = np.where(np.around(this_uplh, sigDig) == np.around(this_qflx.values[0],sigDig))[0]\n",
    "        iCrop  = np.where(np.around(this_uplh, sigDig) == np.around(this_qflx.values[1],sigDig))[0]\n",
    "\n",
    "        # Land-unit means...\n",
    "        iUrban = np.where(np.around(this_uplh, sigDig) == np.around(this_qflx_LU.values[2],sigDig))[0]\n",
    "        iLake  = np.where(np.around(this_uplh, sigDig) == np.around(this_qflx_LU.values[3],sigDig))[0]\n",
    "        \n",
    "        ## TODO: For safety, since using a round off digit, should confirm that things aren't being marked as two PFTs \n",
    "\n",
    "        ## Also fill in if the plume is not active...\n",
    "        this_upa  = scamDS_h2.isel(case=(iCase)).isel(time=iT, ilev=-1).edmf_upa.values\n",
    "        iMiss     = np.where(this_upa==0.0)[0]\n",
    "\n",
    "        plumeLabel[iCase, iT, iGrass] = 'C3grass'\n",
    "        plumeLabel[iCase, iT, iCrop]  = 'IrrigCrop'\n",
    "        plumeLabel[iCase, iT, iLake]  = 'Lake'\n",
    "        plumeLabel[iCase, iT, iUrban] = 'Urban'\n",
    "        plumeLabel[iCase, iT, iMiss]  = 'Off'\n",
    "\n",
    "scamDS_h2['plumeLabel'] = (('case','time','nens'), plumeLabel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff3d75b-7da5-4dbc-80c7-a2db27a2db87",
   "metadata": {},
   "outputs": [],
   "source": [
    "%% time \n",
    "## Get number of plumes at each time/level\n",
    "nPlumesActive = scamDS_h2.edmf_upa_interp.count(dim='nens')\n",
    "print(np.shape(nPlumesActive))\n",
    "\n",
    "scamDS['nPlumesActive'] = (('case','time','levInterp'), nPlumesActive.values)\n",
    "\n",
    "# grassDS = scamDS_h2.where(scamDS_h2.plumeLabel == 'C3grass')\n",
    "# cropDS  = scamDS_h2.where(scamDS_h2.plumeLabel == 'IrrigCrop')\n",
    "# lakeDS  = scamDS_h2.where(scamDS_h2.plumeLabel == 'Lake')\n",
    "# urbanDS = scamDS_h2.where(scamDS_h2.plumeLabel == 'Urban')\n",
    "\n",
    "scamDS['nPlumesActive_grass'] = (('case','time','levInterp'), scamDS_h2.edmf_upa_interp.where(scamDS_h2.plumeLabel == 'C3grass').count(dim='nens').values)\n",
    "scamDS['nPlumesActive_crop']  = (('case','time','levInterp'), scamDS_h2.edmf_upa_interp.where(scamDS_h2.plumeLabel == 'IrrigCrop').count(dim='nens').values)\n",
    "scamDS['nPlumesActive_urban'] = (('case','time','levInterp'), scamDS_h2.edmf_upa_interp.where(scamDS_h2.plumeLabel == 'Urban').count(dim='nens').values)\n",
    "scamDS['nPlumesActive_lake']  = (('case','time','levInterp'), scamDS_h2.edmf_upa_interp.where(scamDS_h2.plumeLabel == 'Lake').count(dim='nens').values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d800e2-f502-47ed-8985-97f2f677e41a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d36d456-b84a-471a-9bce-3c370fd7236c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2607f82b-786a-4779-ba04-1c0482da89ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ae7ce7-65e8-49c5-a53f-0fcfd4f43228",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NPL 2024a",
   "language": "python",
   "name": "npl-2024a"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
